
<!doctype html>
<html lang="en">
  <head>
    <!-- <meta charset="utf-8"> -->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="images/favicon.ico">

    <title>GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">


    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="dns-prefetch" href="//cdn.jsdelivr.net" />
    <link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdn.jsdelivr.net/gh/jablonczay/code-box-copy/code-box-copy/css/code-box-copy.min.css" rel="stylesheet" />

    <style>
        body {
            font-family: 'Source Sans Pro', sans-serif;
            padding-bottom: 50px;
        }
        hr {
            background: #80808083;
        }
        .embed-responsive-2by1 {
            padding-bottom: 50%;
        }
        .embed-responsive-4by1 {
            padding-bottom: 25%;
        }
        .embed-responsive-6by1 {
            padding-bottom: 16.67%;
        }
        .embed-responsive-8by1 {
            padding-bottom: 12.50%;
        }
        .embed-responsive-teaser {
            padding-bottom: 29%;
        }
        .float-button {
            position: fixed;
            right: 1%;
            z-index: 9999;
        }
        .vert {
            transform-origin: 50% 50%;
            transform: rotate(180deg);
            writing-mode: vertical-rl;
            margin: 0;
            margin-left: auto;
            margin-right: 0;
        }
        .caption {
            font-size: 0.9rem;
            margin-top: 0;
            margin-bottom: 5px;
            font-weight: bold;
        }
    </style>
  </head>

  <body>
    <header>
    <main role="main">

      <section class="jumbotron text-center" style="padding: 2%; background-color: #e6e9ec;">
        <div class="container">
          <img src="images/logo.png" style="width: 50%;margin-bottom: 8px; min-width: 350px;">
          <h1 class="jumbotron-heading">Unsupervised 3D Neural Rendering of Minecraft Worlds</h1>
        </div>

        <div class="container" style="max-width: 768px;">
            <div class="row">
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://www.cs.cornell.edu/~zekun/" target="_blank">Zekun Hao<span style="color: #76b900;"><sup>&lowast;</sup></span><span style="color: #B31B1B;"><sup>&dagger;</sup></span></a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://arunmallya.github.io/" target="_blank">Arun Mallya<span style="color: #76b900;"><sup>&lowast;</sup></span></a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://blogs.cornell.edu/techfaculty/serge-belongie/" target="_blank">Serge Belongie<span style="color: #B31B1B;"><sup>&dagger;</sup></span></a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://mingyuliu.net/" target="_blank">Ming-Yu Liu<span style="color: #76b900;"><sup>&lowast;</sup></span></a></h5>
                </div>
            </div>
            <div class="row" style="margin-top:0.75%; margin-left: auto; margin-right: auto;width: 50%;">
                <div class="col-md">
                    <h5 style="color: #76b900; margin-bottom: 0;"><a class="text-center" href="https://www.nvidia.com/en-us/research/" target="_blank" style="color: #76b900;"><b><sup>&lowast;</sup>NVIDIA</b></a></h5>
                </div>
                <div class="col-md">
                    <h5 style="color: #76b900; margin-bottom: 0;"><a class="text-center" href="https://vision.cornell.edu/se3/" target="_blank" style="color: #B31B1B;"><sup>&dagger;</sup>Cornell</a></h5>
                </div>
            </div>
        </div>

        <div class="buttons">
            <a href="#top" target="_blank" class="btn btn-primary my-2">Paper (arxiv)</a>
            <a href="#top" target="_blank" class="btn btn-primary my-2">Paper (embedded videos)</a>
            <a href="#top" target="_blank" class="btn btn-secondary my-2">Code (GitHub)</a>
          </p>
        </div>

        <div class="container" style="max-width: 900px;">
            <div class="row">
                <div class="col-md">
                    <p>We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images from arbitrary viewpoints. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera pose, GANcraft allows user control over both scene semantics and style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis.</p>
                </div>
            </div>
            <!--
            <div class="row">
                <div class="col text-center"><b>Colorization of the world's 3D point cloud</b></div>
                <div class="col text-center"><b>Simultaneously rendered 2D output</b></div>
            </div>
             -->
            <div class="row">
                <div class="embed-responsive embed-responsive-teaser" style="margin: 0;width: 50%;padding-bottom: 25%;">
                    <video controls autoplay muted loop class="embed-responsive-item">
                        <source src="videos/0366_3.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="embed-responsive embed-responsive-teaser" style="margin: 0;width: 50%;padding-bottom: 25%;">
                    <video controls autoplay muted loop class="embed-responsive-item">
                        <source src="videos/3117_2.mp4" type="video/mp4">
                    </video>
                </div>
                </br>
            </div>
            <div class="row">
                <div class="col text-center "><p style="color: #808080; font-size: 0.85rem;">The input block worlds are shown as insets.</p></div>
            </div>

        </div>

      </section>
    </main>

    <!-- TL;DR floating button -->
    <div class="float-button">
        <p class="float-right">
            <a href="#Summary">TL;DR</a>
        </p>
    </div>

    <!-- Summary video -->
    <div class="container" style="max-width: 768px;">
        <h5 class="text-center">Summary Video (TBD)</h5>
        <!--
        <div class="embed-responsive embed-responsive-16by9" style="margin: 0;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/rlCh6-2NfSg" allowfullscreen></iframe>
        </div>
        </br>
        -->
        <h5 class="text-center">Presentation Video (TBD)</h5>
        <!--
        <div class="embed-responsive embed-responsive-16by9" style="margin: 0;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/b2P39sS2kKo" allowfullscreen></iframe>
        </div>
        -->
        <!-- <p class="text-center"><a href="https://www.youtube.com/watch?v=rlCh6-2NfSg" target="_blank">Previous video</a></p> -->
    </div>

    <!-- Overview -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Overview</h2>

                <!-- What is the task of vid2vid? -->
                <h5 class="text-center">What is the problem GANcraft trying to solve?</h5>
                GANcraft aims at solving the world-to-world translation problem. Given a semantically labeled block world such as those from the popular game Minecraft, GANcraft is able to convert it to a new world which shares the same layout but with added photorealism. The new world can then be rendered from arbitrary viewpoints to produce images and videos that are both view consistent and photorealistic. GANcraft simplifies the process of 3D modeling of complex landscape scenes, which will otherwise require years of expertice. GANcraft essentially turns every Minecraft player into a 3D artist.
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col text-center" style="margin-right:50px;"><b>Semantically Labeled Block World</b></div>
            <div class="col text-center" style="margin-left:70px;"><b>Photorealistic Rendering</b></div>
        </div>
        <div class="row">
            <img src="images/vox2img.png" style="width: 100%;margin-bottom: 8px;">
        </div>


        <!-- Issues with prior work. -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">The "Why Don't You Just" Question</h5>
                <!-- One challenge of the world-to-world setting is the nonexistence of ground truth photorealistic renderings for a user-created block world. This suggests that indirect supervision must be used for training such a model. -->
                As the ground truth photorealistic renderings for a user-created block world simply doesn't exist, we have to train such a mode with indirect supervision.
                Some existing approaches are strong candidates. For example, one can use pix2pix methods such as MUNIT and SPADE, originally trained on 2D data only, to convert per-frame segmentation masks, which are projected from the block world, to realistic looking images.
                <!-- However, as these methods do not use 3D information, they are unlikely to produce view-consistent result. -->
                One can also use wc-vid2vid, a 3D-aware method, to generate view-consistent images through 2D inpainting and 3D warping while using the voxel surfaces as the 3D geometry.
                As yet another alternative, one can train a NeRF-W, which learns a 3D radiance field from a non-photometric consistent, but posesd and 3D consistent image collection, and use the results from a pix2pix method, which are the data closest to the requirements that we can get, as the training data.

                <!-- The most relevant prior work in this area is <a class="text-center" href="https://tcwang0509.github.io/vid2vid/" target="_blank">Video-to-Video Synthesis</a>, published at NeurIPS 2018. One of the major shortcomings of this work is that it fails to ensure long-term consistency in the output video. This is because it generates each frame only based on the past few generated frames and lacks knowledge of the structure of the 3D world being generated. Some of the issues are demonstrated in the video below, where we drive forward and then backward to the starting point. -->

                <!-- However, while existing vid2vid methods can maintain short-term temporal consistency, they fail to ensure long-term consistency in the outputs. This is because they generate each frame only based on the past few frames. They lack knowledge of the 3D world being generated. In this work, we propose a framework for utilizing all past generated frames when synthesizing each frame. This is achieved by condensing the 3D world generated so far into a physically-grounded estimate of the current frame, which we call the <span style="background-color: #ffff00d5"><em><strong>guidance image</strong></em></span>. A novel Multi-SPADE module is also proposed to take advantage of the information stored in the guidance images. Extensive experimental results on several challenging datasets verify the effectiveness of our method in achieving world consistency â€“ the output video is consistent within the entire generated 3D world. -->
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col text-center" style="margin-right:0px;">MUNIT</div>
            <div class="col text-center" style="margin-left:0px;">SPADE</div>
            <div class="col text-center" style="margin-left:0px;">wc-vid2vid</div>
            <div class="col text-center" style="margin-left:0px;">NSVF-W</div>
            <div class="col text-center" style="margin-left:0px;"><b>GANcraft(ours)</b></div>
        </div>
        <div class="row">
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/survivalisland_2467_1.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/landscapep2_0685_3.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/s123456_0298_1.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/desert_2527_2.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col-md-12">
                Comparing the results from different methods, we can immediately notice a few issues:
                <ul>
                    <li>Pix2pix methods such as MUNIT and SPADE does not preserve viewpoint consistency, as these methods have no knowledge of the 3D geometry, and each frame is generated independently.</li>
                    <li>Wc-vid2vid produces view-consistent video, but the image quality deterorates quickly with time due to error accumulation.</li>
                    <li>NSVF-W (our implementation of NeRF-W with added voxel conditioning) produces view-consistent output as well, but the result looks dull and lacks fine detail. The NSVF-W is trained with images produced by SPADE, which lack multi-view consistency. This have attributed to the lower image fidelity.</li>
                </ul>
                In the last column, we present results from GANcraft, which are both view-consistent and high-quality. The use of neural rendering guarantees view-consistency, while the innovation in model architecture and training scheme leads to unprecedented photorealism.
            </div>
        </div>

        <!-- Training with Pseudo-ground truth. -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">Distribution Mismatch and Pseudo-ground truth</h5>
                <p>Assume that we have a suitable voxel-conditional neural rendering model which is capable of representing the photorealistic world. We still need a way to train it without any ground truth posed images. Adversarial training has achieved some success in small scale, unconditional neural rendering tasks when the posed images are not available. However, for  GANcraft the problem is even more challenging. The block worlds from Minecraft usually have wildly different label distributions compared to the real world. Some scenes are completetly covered by snow. There are also scenes that cross multiple biomes within a small area. Moreover, it is impossible to match the sampled camera distribution to that of internet photos when randomly sampling views from the neural rendering model.</p>
                <!-- <p>Show ablation image. Shows that the problem is under-constrained and GAN loss alone does not work.</p> -->
            </div>
        </div>
        <div class="row">
            <div style="width: 3em;">
                <p class="vert"><b>W/ pseudo-ground truth</b></p>
            </div>
            <img src="images/gancraft/00041.png" style="width: 22%;margin-bottom: 8px;">
            <img src="images/gancraft/00046.png" style="width: 22%;margin-bottom: 8px;">
            <!-- <img src="images/gancraft/00129.png" style="width: 20%;margin-bottom: 8px;"> -->
            <img src="images/gancraft/00461.png" style="width: 22%;margin-bottom: 8px;">
            <img src="images/gancraft/01069.png" style="width: 22%;margin-bottom: 8px;">
        </div>
        <div class="row">
            <div style="width: 3em;">
                <p class="vert">No pseudo-ground truth</p>
            </div>
            <img src="images/nopseudo/00041.png" style="width: 22%;margin-bottom: 8px;">
            <!-- <img src="images/nopseudo/00046.png" style="width: 20%;margin-bottom: 8px;"> -->
            <img src="images/nopseudo/00129.png" style="width: 22%;margin-bottom: 8px;">
            <img src="images/nopseudo/00461.png" style="width: 22%;margin-bottom: 8px;">
            <img src="images/nopseudo/01069.png" style="width: 22%;margin-bottom: 8px;">
        </div>
        <div class="row">
            <div class="col-md-12">
                <p>As shown in the second row, adversarial training along using internet photos leads to unrealistic results. As one of our main contributions, we discovered that incorporating pseudo-ground truth into the training significantly improves the result (first row).</p>

                <img class="mx-auto d-block" src="images/pgt_generation.svg" style="width: 70%;margin-bottom: 8px;">
                <p class="caption text-center">Generating pseudo-ground truths</p>

                <p>The pseudo-ground truths are the photorealistic images generated from segmentation masks using a pretrained SPADE model. As the segmentation masks are sampled from the block world, the pseudo-ground truths share the same labels and camera poses as the images generated from the same views. This not only reduces label and camera distribution mismatch, but also allows us to use stronger losses such as perceptual loss for faster and more stable training.</p>
            </div>
        </div>


        <!-- Hybird Voxel-conditional Neural Rendering -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">Hybird Voxel-conditional Neural Rendering</h5>
                <p>In GANcraft, we represent the photorealistic scene with a combination of 3D volumetric renderer and 2D image space renderer. We define a voxel-bounded neural radiance field: given a block world, we assign a learnable feature vector to every corner of the blocks, and use trilinear interpolation to define the location code at arbitrary locations within a voxel. A radiance field can then be defined implicitly using an MLP, which takes the location code, semantic lable and a shared style code as input and produces a point feature (radiance) and its volume density. Given camera parameters, we rendering the radiance field to obtain a 2D feature map, which is further converted to an image via a CNN.</p>
                <img class="mx-auto d-block" src="images/pipeline_overview_w_skydome.svg" style="width: 100%;margin-bottom: 8px;">
                <p class="caption text-center">The complete GANcraft architecture</p>
                The two-stage architecture significantly improves the image quality while reducing the computation and memory footprint, as the radiance field can be modeled with a simpler MLP, which is the computational bottleneck for implicit volume based methods. The proposed architecture is capable of handling very large worlds. In our experiments, we use voxel grids with a size of 512&times;512&times;256, which is equivalent to 65 acres or 32 soccer fields in real world.
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">Neural Sky Dome</h5>
                <p>Previous voxel-based neural rendering approaches cannot model sky that is infinitely far away. However, sky is an indispensable ingredient for photorealism. In GANcraft, we use an additional MLP to model sky. The MLP converts the camera ray direction to a feature vector which has the same dimension as the point features from the radiance field. This feature vector serves as the totally opaque, final sample on a ray, blending into the pixel feature according to the residual transmittance of the ray.</p>
            </div>
        </div>


        <!-- Style -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">Generating Images with Diversified Appearance</h5>
                <p>The generation process of GANcraft is conditional on a style image. During training, we use the pseudo-ground truth as the style image, which helps explain away the inconsistency between the generated image and its corresponding pseudo-ground truth for the reconstruction loss. During evaluation, we can control the output style by providing GANcraft with different style images.</p>
                <div class="embed-responsive embed-responsive-16by9" style="margin: 0;width: 100%;">
                    <video controls muted loop class="embed-responsive-item">
                        <source src="videos/interp_small.mp4" type="video/mp4">
                    </video>
                </div>
                <p class="caption text-center">Style Control Example.</p>
                <p>In the example above, we linearlly interpolate the style code across 6 different style images. </p>
            </div>
        </div>
    </div>

    <!-- Outputs. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Additional Results</h2>
            </div>
        </div>

        <div style="width: 75%; margin: 0 auto;">
            <div class="embed-responsive embed-responsive-2by1" style="width: 100%; margin: 0 auto;">
                <iframe class="embed-responsive-item" style="padding: 0;" src="videos/stuttgart_01_06.mp4" allowfullscreen></iframe>
            </div></br>
            <div class="embed-responsive embed-responsive-2by1" style="width: 100%; margin: 0 auto;">
                <iframe class="embed-responsive-item" style="padding: 0;" src="videos/mannequin.mp4" allowfullscreen></iframe>
            </div></br>
        </div>

    </div>

    <!-- Summary. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;" id="Summary">
        <div class="row">
            <div class="col-md-12">
                <h2>Summary</h2>
                <ul>
                    <li>GANcraft is a powerful tool for converting semantic block worlds to photorealistic worlds without the need of ground truth data.</li>
                    <li>Existing methods perform poorly on the task due to the lack of viewpoint consistency and photorealism.</li>
                    <li>GANcraft performs well in this challenging world-to-world setting where the ground truth is unavailable and the distribution mismatch between a Minecraft world and internet photos is significant.</li>
                    <li>We introduce a new training scheme which uses pseudo-ground truth. This improves the quality of the results significantly.</li>
                    <li>We introduce a hybrid neural rendering pipeline which is able to represent large and complex scenes efficiently.</li>
                    <li>We are able to control the appearance of the GANcraft results using style images.</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Citation. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
            </div>
        </div>
    <div class="code-box-copy"  style="font-size: 0.9rem !important; max-width: 768px; margin: 0 auto;">
        <button class="code-box-copy__btn" data-clipboard-target="#citation" title="Copy" style="opacity: 1;"></button>
        <pre><code class="language-bib" style="font-size: 0.9rem;" id="citation">@inproceedings{tbd2021tbd,
    title={GANcraft: Unsupervised 3D Neural Rendering of MinecraftWorlds},
    author={Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
    booktitle={TBD TBD TBD TBD},
    year={2021}
}</code></pre>
    </div>
    </div>


    <footer class="text-muted">
      <div class="container">
        <p class="float-right">
          <a href="#">Back to top</a>
        </p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/combine/gh/jablonczay/code-box-copy/clipboard/clipboard.min.js,gh/jablonczay/code-box-copy/code-box-copy/js/code-box-copy.min.js"></script>
    <script src="https://saswatpadhi.github.io/prismjs-bibtex/prism-bibtex.min.js"></script>
    <script>
        (function($) {
            $('.code-box-copy').codeBoxCopy();
        })(jQuery);
    </script>
  </body>
</html>
