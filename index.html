
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="images/favicon.ico">

    <title>GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">


    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="dns-prefetch" href="//cdn.jsdelivr.net" />
    <link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdn.jsdelivr.net/gh/jablonczay/code-box-copy/code-box-copy/css/code-box-copy.min.css" rel="stylesheet" />

    <style>
        body {
            font-family: 'Source Sans Pro', sans-serif;
            padding-bottom: 50px;
        }
        hr {
            background: #80808083;
        }
        .embed-responsive-2by1 {
            padding-bottom: 50%;
        }
        .embed-responsive-4by1 {
            padding-bottom: 25%;
        }   
        .embed-responsive-6by1 {
            padding-bottom: 16.67%;
        }
        .embed-responsive-8by1 {
            padding-bottom: 12.50%;
        }
        .embed-responsive-teaser {
            padding-bottom: 29%;
        }
        .float-button {
            position: fixed;
            right: 1%;
            z-index: 9999;
        }
    </style>
  </head>

  <body>
    <header>
    <main role="main">

      <section class="jumbotron text-center" style="padding: 2%; background-color: #e6e9ec;">
        <div class="container">
          <img src="images/logo.png" style="width: 50%;margin-bottom: 8px;">
          <h1 class="jumbotron-heading">GANcraft<br>Unsupervised 3D Neural Rendering of Minecraft Worlds</h1>
        </div>

        <div class="container" style="max-width: 768px;">
            <div class="row">
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://www.cs.cornell.edu/~zekun/" target="_blank">Zekun Hao</a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://arunmallya.github.io/" target="_blank">Arun Mallya</a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://blogs.cornell.edu/techfaculty/serge-belongie/" target="_blank">Serge Belongie</a></h5>
                </div>
                <div class="col-md">
                    <h5 id="authors" class="text-center" style="margin: 0px;"><a class="text-center" href="http://mingyuliu.net/" target="_blank">Ming-Yu Liu</a></h5>
                </div>
            </div>

            <div class="row" style="margin-left: auto; margin-right: auto;width: 50%;">
                <div class="col-md">
                    <h5 style="color: #76b900; margin-bottom: 0;"><a class="text-center" href="https://vision.cornell.edu/se3/" target="_blank" style="color: #B31B1B;font-family: serif;">Cornell</a></h5>
                </div>
                <div class="col-md">
                    <h5 style="color: #76b900; margin-bottom: 0;"><a class="text-center" href="https://www.nvidia.com/en-us/research/" target="_blank" style="color: #76b900;"><b>NVIDIA</b></a></h5>
                </div>
            </div>
        </div>

        <div class="buttons">
            <a href="#top" target="_blank" class="btn btn-primary my-2">Paper (arxiv)</a>
            <a href="#top" target="_blank" class="btn btn-primary my-2">Paper (embedded videos)</a>
            <a href="#top" target="_blank" class="btn btn-secondary my-2">Code (GitHub)</a>
          </p>
        </div>

        <div class="container" style="max-width: 900px;">
            <div class="row">
                <div class="col-md">
                    <p>We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images from arbitrary viewpoints. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera pose, GANcraft allows user control over both scene semantics and style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis.</p>
                </div>
            </div>
            <!-- 
            <div class="row">
                <div class="col text-center"><b>Colorization of the world's 3D point cloud</b></div>
                <div class="col text-center"><b>Simultaneously rendered 2D output</b></div>
            </div>
             -->
            <div class="row">
                <div class="embed-responsive embed-responsive-teaser" style="margin: 0;width: 50%;padding-bottom: 25%;">
                    <video controls autoplay muted loop class="embed-responsive-item">
                        <source src="videos/0366_3.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="embed-responsive embed-responsive-teaser" style="margin: 0;width: 50%;padding-bottom: 25%;">
                    <video controls autoplay muted loop class="embed-responsive-item">
                        <source src="videos/3117_2.mp4" type="video/mp4">
                    </video>
                </div>
                </br>
            </div>
            <div class="row">
                <div class="col text-center "><p style="color: #808080; font-size: 0.85rem;">The input block worlds are shown as insets.</p></div>
            </div>
            
        </div>

      </section>
    </main>

    <!-- TL;DR floating button -->
    <div class="float-button">
        <p class="float-right">
            <a href="#Summary">TL;DR</a>
        </p>
    </div>

    <!-- Summary video -->
    <div class="container" style="max-width: 768px;">
        <h5 class="text-center">Summary Video (TBD)</h5>
        <!--
        <div class="embed-responsive embed-responsive-16by9" style="margin: 0;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/rlCh6-2NfSg" allowfullscreen></iframe>
        </div>
        </br>
        -->
        <h5 class="text-center">Presentation Video (TBD)</h5>
        <!--
        <div class="embed-responsive embed-responsive-16by9" style="margin: 0;">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/b2P39sS2kKo" allowfullscreen></iframe>
        </div>
        -->
        <!-- <p class="text-center"><a href="https://www.youtube.com/watch?v=rlCh6-2NfSg" target="_blank">Previous video</a></p> -->
    </div>

    <!-- Overview -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Overview</h2>

                <!-- What is the task of vid2vid? -->
                <h5 class="text-center">What is the problem GANcraft trying to solve?</h5>
                GANcraft aims at solving the world-to-world translation problem. Given a semantically labeled block world such as those from the popular game Minecraft, GANcraft is able to convert it to a new world which shares the same layout but with added photorealism. The new world can then be rendered from arbitrary viewpoints to produce images and videos that are both view consistent and photorealistic. GANcraft simplifies the process of 3D modeling of complex landscape scenes, which will otherwise require years of expertice. GANcraft essentially turns every Minecraft player into a 3D artist.
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col text-center" style="margin-right:50px;"><b>Semantically Labeled Block World</b></div>
            <div class="col text-center" style="margin-left:70px;"><b>Photorealistic Rendering</b></div>
        </div>
        <div class="row">
            <img src="images/vox2img.png" style="width: 100%;margin-bottom: 8px;">
        </div>


        <!-- Issues with prior work. -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">The "Why Don't You Just" Question</h5>
                <!-- One challenge of the world-to-world setting is the nonexistence of ground truth photorealistic renderings for a user-created block world. This suggests that indirect supervision must be used for training such a model. -->
                As the ground truth photorealistic renderings for a user-created block world simply doesn't exist, we have to train such a mode with indirect supervision.
                Some existing approaches are strong candidates. For example, one can use pix2pix methods such as MUNIT and SPADE, originally trained on 2D data only, to convert per-frame segmentation masks, which are projected from the block world, to realistic looking images. 
                <!-- However, as these methods do not use 3D information, they are unlikely to produce view-consistent result. -->
                One can also use wc-vid2vid, a 3D-aware method, to generate view-consistent images through 2D inpainting and 3D warping while using the voxel surfaces as the 3D geometry. As yet another alternative, one can train a NeRF-W, which learns a 3D radiance field from a non-photometric consistent image collection, and use the results from a pix2pix method as the training data.
                
                <!-- The most relevant prior work in this area is <a class="text-center" href="https://tcwang0509.github.io/vid2vid/" target="_blank">Video-to-Video Synthesis</a>, published at NeurIPS 2018. One of the major shortcomings of this work is that it fails to ensure long-term consistency in the output video. This is because it generates each frame only based on the past few generated frames and lacks knowledge of the structure of the 3D world being generated. Some of the issues are demonstrated in the video below, where we drive forward and then backward to the starting point. -->

                <!-- However, while existing vid2vid methods can maintain short-term temporal consistency, they fail to ensure long-term consistency in the outputs. This is because they generate each frame only based on the past few frames. They lack knowledge of the 3D world being generated. In this work, we propose a framework for utilizing all past generated frames when synthesizing each frame. This is achieved by condensing the 3D world generated so far into a physically-grounded estimate of the current frame, which we call the <span style="background-color: #ffff00d5"><em><strong>guidance image</strong></em></span>. A novel Multi-SPADE module is also proposed to take advantage of the information stored in the guidance images. Extensive experimental results on several challenging datasets verify the effectiveness of our method in achieving world consistency – the output video is consistent within the entire generated 3D world. -->
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col text-center" style="margin-right:0px;">MUNIT</div>
            <div class="col text-center" style="margin-left:0px;">SPADE</div>
            <div class="col text-center" style="margin-left:0px;">wc-vid2vid</div>
            <div class="col text-center" style="margin-left:0px;">NSVF-W</div>
            <div class="col text-center" style="margin-left:0px;"><b>GANcraft(ours)</b></div>
        </div>
        <div class="row">
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/survivalisland_2467_1.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/landscapep2_0685_3.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/s123456_0298_1.mp4" type="video/mp4">
                </video>
            </div>
            <div class="embed-responsive" style="padding-bottom: 20%; margin: 0;">
                <video controls autoplay muted loop class="embed-responsive-item">
                    <source src="videos/desert_2527_2.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        </br>
        <div class="row">
            <div class="col-md-12">
                Comparing the results from different methods, we can immediately notice a few issues:
                <ul>
                    <li>Pix2pix methods such as MUNIT and SPADE does not preserve viewpoint consistency, as these methods have no knowledge of the 3D geometry, and each frame is generated independently.</li>
                    <li>Wc-vid2vid produces view-consistent video, but the image quality deterorates quickly with time due to error accumulation.</li>
                    <li>NSVF-W (our implementation of NeRF-W with added voxel conditioning) produces view-consistent output as well, but the result looks dull and lacks fine detail. The NSVF-W is trained with the images produced by SPADE, which lack multi-view consistency. This has attributed to the low image fidelity.</li>
                </ul>
                In the last column, we present results from GANcraft, which are both view-consistent and high-quality.
            </div>
        </div>

        <!-- Introduction to guidance images. -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">Achieving consistency across views and time</h5>
                <p>We believe that in order to produce realistic outputs that are consistent over time and viewpoint change, the method must be aware of the 3D structure of the world.
                To achieve this, we introduce the concept of <span style="background-color: #ffff00d5"><em><strong>guidance images</strong></em></span>, which are physically-grounded estimates of what the next output frame should look like, based on how the world has been generated so far. As alluded to in their name, the role of these <span style="background-color: #ffff00d5"><em><strong>guidance images</strong></em></span> is to guide the generative model to produce colors and textures that respect previous outputs.</p>

                <p>While prior works use optical flow to warp prior outputs, our guidance image differs from this in two aspects.
                First, instead of using optical flow, the guidance image should be generated by using the motion field, or scene flow, which describes the true motion of each 3D point in the world. Second, the guidance image should aggregate information from all past viewpoints (and thus frames), instead of only the direct previous frames as in vid2vid. This makes sure that the generated frame is consistent with the entire history.</p>

                <p>The figure below shows one method to generate guidance images by using point clouds and camera locations obtained by performing <a class="text-center" href="https://en.wikipedia.org/wiki/Structure_from_Motion#:~:text=Structure%20from%20Motion%20(SfM)%20is,computer%20vision%20and%20visual%20perception." target="_blank">Structure from Motion (SfM)</a> on an input video. In case of a game rendering engine, the ground truth scene flow can be obtained and used to generate guidance images.</p>


                <img src="images/guidance.png" style="width: 100%;margin-bottom: 8px;">
                <div class="text-center">
                    <p style="font-size: 0.9rem; margin-bottom: 0;"><strong>Generating 3D-aware guidance images</strong></p>
                    <p style="color: #808080; font-size: 0.85rem;">
                        A camera(s) with known parameters and positions travels over time \( t = 0,\cdots,N \). At \( t = 0 \), the scene is textureless and an output image is generated for this viewpoint. The output image is then back-projected to the scene and a guidance image for a subsequent camera position is generated by projecting the partially textured point cloud. Using this <span style="background-color: #ffff00d5"><em><strong>guidance image</strong></em></span>, the generative method can produce an output that is consistent across views and smooth over time.
                        The guidance image can be noisy, misaligned, and have holes, and the generation method should be robust to such inputs.
                    </p>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <p>As input to our model, we provide the guidance images along with the input labels such as semantic segmentation and depth maps. The inputs and generated outputs are visualized in the below video. As new 3D points become visible to the camera, colors are assigned to them by our image generator and the point cloud is updated. Note that the guidance images have holes and incorrect projections due to noisy point cloud information. Our method is robust to such noise and produces meaningful outputs.</p>
            </div>
        </div>
        <div class="row">
            <div class="col text-center"><b>Semantic Segmentation</b></div>
            <div class="col text-center"><b>Depth</b></div>
            <div class="col text-center"><span style="background-color: #ffff00d5"><strong>Guidance Image</strong></span></div>
            <div class="col text-center"><b>Rendered Video</b></div>
        </div>
        <div class="row">
            <div class="embed-responsive embed-responsive-8by1" style="margin: 0;">
                <video controls loop class="embed-responsive-item">
                    <source src="videos/what_is_guidance.mp4" type="video/mp4">
                </video>
            </div>
        </div></br>

        <div class="row">
            <div class="col-md-12">
                <p>Here, we show how the application of our method solves the issues with temporal consistency observed above with prior work. As the viewer returns back to the starting position, the produced output is very similar to that of the first image, respecting previously produced textures. The output images and transitions over time also look more realistic.</p>
            </div>
        </div>
        <div class="row">
            <div class="embed-responsive embed-responsive-4by1" style="margin: 0;">
                <video controls class="embed-responsive-item">
                    <source src="videos/fb_guidance.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <br>

        <!-- The network architecture. -->
        <div class="row">
            <div class="col-md-12">
                <hr style="max-width: 768px;">
                <h5 class="text-center">The complete network architecture</h5>

                <p>Here, we visualize our generator architecture and all its components. The main component in our generator is the novel Multi-SPADE block, which is composed of multiple <a class="text-center" href="https://nvlabs.github.io/SPADE/" target="_blank">SPADE</a> layers. Each SPADE layer takes in a spatial conditioning map such as the semantic segmentation, or the optical flow-warped previous output, or the guidance image, and applies appropriate transformations on the intermediate feature maps so that the finally generated output respects the required constraints and looks realistic in the spatial and temporal domain.
                </p>

                <div class="text-center">
                    <img src="images/overview.png" style="width: 100%;margin-bottom: 8px;">
                    <p style="font-size: 0.9rem; margin-bottom: 0;"><strong>The overall architecture based on the Multi-SPADE module</strong></p>
                    <p style="color: #808080; font-size: 0.85rem;">
                        Each Multi-SPADE module takes input label features, warped previous frame features, and guidance images to modulate the features in each layer of our generator. The <span style="background-color: #ff88009c">labels decide the semantic content of the output frames</span>, the <span style="background-color: #00ff37a2">optical-flow warped previous frames ensure short-term consistency</span>, and the <span style="background-color: #ff00d469">guidance images ensure long-term consistency</span>.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <!-- Outputs. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Sample Video Generation Results</h2>
            </div>
        </div>

        <div style="width: 75%; margin: 0 auto;">
            <div class="embed-responsive embed-responsive-16by9" style="width: 100%; margin: 0 auto;">
                <iframe class="embed-responsive-item" style="padding: 0;" src="videos/interp_small.mp4" allowfullscreen></iframe>
            </div></br>
            <div class="embed-responsive embed-responsive-2by1" style="width: 100%; margin: 0 auto;">
                <iframe class="embed-responsive-item" style="padding: 0;" src="videos/stuttgart_01_06.mp4" allowfullscreen></iframe>
            </div></br>
            <div class="embed-responsive embed-responsive-2by1" style="width: 100%; margin: 0 auto;">
                <iframe class="embed-responsive-item" style="padding: 0;" src="videos/mannequin.mp4" allowfullscreen></iframe>
            </div></br>
        </div>

        <hr style="max-width: 768px;">
        <p class="text-center" style="margin-bottom: 0;">
            <h5 class="text-center">Consistent Multiview Generation</h5>
            We can also simultaneously generate videos that are consistent across multiple viewpoints or users.
            In this setting, the first user generates the first frame and appropriate colors are assigned to the world's 3D point cloud. The 3D point cloud is then splatted/projected to the viewpoint of the second user to be used as guidance and our network generates the final semantically consistent output image after resolving holes and errors. Colors are then assigned to the newly visible 3D points and the point cloud is updated.
            Below are examples of stereo pair generations, with the point cloud being alternately updated by the left and right views.
        </p>
        <div class="embed-responsive embed-responsive-4by1" style="width: 100%; margin: 0 auto;">
            <iframe class="embed-responsive-item" style="padding: 0;" src="videos/stereo_1.mp4" allowfullscreen></iframe>
        </div></br>
        <div class="embed-responsive embed-responsive-4by1" style="width: 100%; margin: 0 auto;">
            <iframe class="embed-responsive-item" style="padding: 0;" src="videos/stereo_2.mp4" allowfullscreen></iframe>
        </div>
    </div>

    <!-- Summary. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;" id="Summary">
        <div class="row">
            <div class="col-md-12">
                <h2>Summary</h2>
                <ul>
                    <li>Video-to-video synthesis is a powerful tool for converting semantic inputs to photorealistic videos.</li>
                    <li>Existing vid2vid methods are unable to maintain long-term consistency (such as during loop closure) due to lack of the 3D structure of the world.</li>
                    <li>We provide information about the 3D structure of the world using guidance images - projecting point clouds of the world colored so far (which can be noisy and incomplete) to the of the current camera view.</li>
                    <li>We introduce a new architecture based on the Multi-SPADE module, which uses semantic labels, optical-flow warping, and guidance images as conditioning inputs.</li>
                    <li>Our models improve upon the realism, short-term, and long-term consistency of generated videos.</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Citation. -->
    <hr style="max-width: 768px;">
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
            </div>
        </div>
    <div class="code-box-copy"  style="font-size: 0.9rem !important; max-width: 768px; margin: 0 auto;">
        <button class="code-box-copy__btn" data-clipboard-target="#citation" title="Copy" style="opacity: 1;"></button>
        <pre><code class="language-bib" style="font-size: 0.9rem;" id="citation">@inproceedings{mallya2020world,
    title={World-Consistent Video-to-Video Synthesis},
    author={Arun Mallya and Ting-Chun Wang and Karan Sapra and Ming-Yu Liu},
    booktitle={Proceedings of the European Conference on Computer Vision},
    year={2020}
}</code></pre>
    </div>
    </div>


    <footer class="text-muted">
      <div class="container">
        <p class="float-right">
          <a href="#">Back to top</a>
        </p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/combine/gh/jablonczay/code-box-copy/clipboard/clipboard.min.js,gh/jablonczay/code-box-copy/code-box-copy/js/code-box-copy.min.js"></script>
    <script src="https://saswatpadhi.github.io/prismjs-bibtex/prism-bibtex.min.js"></script>
    <script>
        (function($) {
            $('.code-box-copy').codeBoxCopy();
        })(jQuery);
    </script>
  </body>
</html>
